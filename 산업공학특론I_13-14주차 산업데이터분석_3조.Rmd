---
title: "산업공학특론I_13-14주차_산업데이터분석"
output: html_document
date: "2024-05-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>
<br>
<br>

## [데이터 분석 개요]
### 1. 대상 데이터: 다단계 공정 데이터셋
(https://www.kaggle.com/datasets/supergus/multistage-continuousflow-manufacturing-process)

미시간 주 디트로이트 근처의 실제 생산 라인 내에서 여러 위치, 여러 생산 라인에 대하여 계측한 데이터

다양한 입력 데이터로부터 라인의 출력 특정 속성을 예측하기 위한 목적으로 수집

해당 공정 라인은 병렬 및 직렬 단계로 구성된 고속 연속 제조 공정으로, 다음과 같은 구조를 가짐

* Stage 1에서는 M1, M2, M3 기계가 병렬로 작동하며, 이들의 출력이 결합기로 전달

* 결합기에서 출력되는 Stage 1에 대한 예측치는 제작된 제품의 외부 표면을 둘러싼 15개의 위치에서 측정

* 다음으로, M4와 M5가 직렬로 처리하는 Stage 2로 이동

* M5 작동 후에는 동일한 15개의 위치에 대한 Stage 2의 측정이 이루어짐

<br>

![](w13-14_fig.png)

<br>
* Oleghe, O. (2020). A predictive noise correction methodology for manufacturing process datasets. Journal of Big Data, 7(1), 89.

<br>
<br>
<br>

### 2. 분석 절차

데이터 탐색 및 전처리 / 특징 추출 및 차원 축소 / 모델 학습 및 평가 순서로 3단계로 나누어 분석 진행

2단계 공정인 관계로 Stage 1에 대한 예측모델, Stage 2에 대한 예측모델을 2단계로 모델링할 것 (이 때, Stage 2의 결과는 Stage 1에 영향을 받음)

원래 Stage 1, Stage 2에 대한 예측값은 각각 15개씩이지만, 본 강의에서는 다변량 예측을 다루지 않는 관계로 각 Stage에 대한 평균값을 종속변수로 설정

팀별로 작업을 수행하며, 작업 코드는 팀장의 Github에 Push하여 공유, 통찰력 있는 분석 기법 발굴 시 모두와 공유

가이드로 제공된 참고문헌을 바탕으로 다양한 방법론으로 분석을 수행

<br>
<br>
<br>

## [데이터 분석]
### 1. 데이터 탐색 및 전처리

* 데이터의 전반적인 분포, 특징을 파악하기 위한 기초 분석 진행

* 다양한 전처리 방법론을 활용하여 데이터를 정제
(e.g. 필요없는 변수 및 이상치/노이즈 제거 또는 보정, 표준화 등)

* 데이터 전처리 순서 (학습, 테스트셋 분할 전후)를 꼼꼼히 살펴보고 진행할 것

```{r preprocess}


```

<br>

### 2. 특징 추출 및 차원 축소

* 전처리가 이루어진 데이터로부터 특성을 재정의하거나 차원 축소 기법을 적용

* 신규 변수 또는 축소된 차원으로 효과적인 예측을 수행하기 위한 방안 도출

```{r feature}
# 데이터 불러오기
train_s1 <- read.csv('train_Stage1.csv')
train_s2 <- read.csv('train_Stage2.csv')
test_s1 <- read.csv('test_Stage1.csv')
test_s2 <- read.csv('test_Stage2.csv')

library(ggplot2)

# Perform PCA
pca_result1 <- prcomp(train_s1, scale. = TRUE)
pca_result2 <- prcomp(train_s2, scale. = TRUE)
pca_result3 <- prcomp(test_s1, scale. = TRUE)
pca_result4 <- prcomp(test_s2, scale. = TRUE)

# Summary of PCA results
summary(pca_result1)
summary(pca_result2)
summary(pca_result3)
summary(pca_result4)

# PCA scores (principal components)
pca_scores_12 <- pca_result$x[, 1:12]

colnames(train_s1)

# Add the species column back to the PCA scores for visualization
pca_scores_12 <- data.frame(pca_scores_12, Stage1_Output = train_s1$Stage1_Output)

# Visualize the PCA results
ggplot(pca_scores_12, aes(x = PC1, y = PC2, color = Stage1_Output)) +
  geom_point(size = 2) +
  labs(title = "PCA of train_s1", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal()

library(caret)
library(e1071)
library(pls)

pca <- preProcess(train_s1[, -ncol(train_s1)], method = 'pca', pcaComp=12)
 trainpca <- predict(pca, train_s1[,-ncol(train_s1)])
 testpca <- predict(pca, test_s1[,-ncol(test_s1)])
 trainpca$Stage1_Output <- train_s1$Stage1_Output
 testpca$Stage1_Output <- test_s1$Stage1_Output


```

<br>

### 3. 모델 학습 및 평가

* Stage 1, Stage 2에 대한 예측 모델을 수립할 것

* 이 때, 각 Stage는 연결되어 있으며 Stage 2는 Stage 1의 영향을 받음

```{r modeling}
# 의사결정나무
library(rpart)
train_control <- trainControl(method = 'cv', number = 10)
rpart_control <- rpart.control(maxdepth=5, minsplit=2) 
dt_grid <- expand.grid(.cp = seq(0.01, 0.1, by = 0.01))
dt <- train(Stage1_Output ~ ., data = train_s1, method = 'rpart', trControl = train_control, tuneGrid = dt_grid, control=rpart_control)

dtpca <- train(Stage1_Output ~ ., data = trainpca, method = 'rpart', trControl = train_control, tuneGrid = dt_grid, control=rpart_control)
dtpca

library(rpart.plot)
par(mfrow=c(1,2))
rpart.plot(dt$finalModel)
rpart.plot(dtpca$finalModel)

# 다중회귀

reg = lm(Stage1_Output ~ ., data = trainpca)
summary(reg)


# 랜덤포레스트
library(randomForest)
rf_model <- randomForest(x = trainpca[,-13], y = trainpca[,13], ntree = 50)
print(rf_model)
```
